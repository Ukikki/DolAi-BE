import base64
import numpy as np
import uuid
import os
import asyncio
import re
from fastapi import FastAPI, WebSocket
from faster_whisper import WhisperModel
from datetime import datetime
from dotenv import load_dotenv
import requests
import webrtcvad

from hallucination_remover import HallucinationRemover

load_dotenv()

SPRING_URL = "https://3.34.92.187.nip.io/api/stt/log"
AZURE_TRANSLATOR_KEY = os.getenv("AZURE_TRANSLATOR_KEY")
AZURE_TRANSLATOR_REGION = os.getenv("AZURE_TRANSLATOR_REGION")
AZURE_TRANSLATOR_ENDPOINT = "https://api.cognitive.microsofttranslator.com"

# í™˜ê° ì œê±°ê¸° ì„¤ì •
remover = HallucinationRemover(
    stopwords=[
        # ì¼ë³¸ì–´ í™˜ê°
        "ã¯ã„", "ã«ã‚“ã˜ã‚“", "ã«ã«ã‚“ãŒ", "ã“ã‚“ã«ã¡ã¯", "ã‚ã‚ŠãŒã¨ã†", "ãŠã¯ã‚ˆã†",
        "ã•ã‚ˆã†ãªã‚‰", "ã™ã¿ã¾ã›ã‚“", "ã”ã‚ã‚“ãªã•ã„", "ã“ã‚“ã«ã¡ã¯ã‚ˆ",
        # ì˜ë¯¸ì—†ëŠ” ì˜ì–´ í™˜ê°
        "little", "good.", "Oh,", "oh", "yeah", "yes", "okay",
        # ê¸°íƒ€ í™˜ê°
        "ã…", "í•³", "ì‹œì²­ì ì—¬ëŸ¬ë¶„"
    ],
    allowed_languages=["korean", "english", "chinese"]
)

TODO_KEYWORDS = [
    "í•´ì¤˜", "í•´ì£¼ì„¸ìš”", "í•´ ì£¼ì„¸ìš”", "ì£¼ì„¸ìš”", "ì˜¤ì„¸ìš”", "í•´ ì£¼ì‚¼", "í•´ ì¤˜", "í•´ì•¼ í•´", "ì™„ë£Œí•´", "ì²˜ë¦¬í•´", "í•´ë³¼ê²Œ", "í• ê²Œìš”",
    "í•  í•„ìš”ê°€ ìˆì–´", "í•„ìš”í•©ë‹ˆë‹¤", "ì¡°ì¹˜", "ì§„í–‰", "ì •ë¦¬í•´", "í™•ì¸í•´", "í•´ì•¼ê² ì–´",
    "í•´ì•¼ê² ë‹¤", "ì¼ì •", "ë§ˆê°", "í•˜ì„¸ìš”", "ì¶”ê°€í•´", "ë“±ë¡í•´", "ë‚¨ê²¨", "ì •ë¦¬",
    "assign", "need to", "must", "should", "complete", "submit", "handle", "todo"
]
BLOCK_PATTERNS = [
    "êµ¬ë…", "ì¢‹ì•„ìš”", "ì•Œë¦¼ì„¤ì •", "ì±„ë„",
]

app = FastAPI()
vad = webrtcvad.Vad(2)
model = WhisperModel("small", device="cpu", compute_type="int8")

# ğŸ¯ í”„ë¡¬í”„íŠ¸ ì„¤ì •
WHISPER_PROMPT_AUTO = """Korean, Chinese, and English business meeting. The sentence may be cut off, do not make up words to fill in the rest of the sentence."""
WHISPER_PROMPT_KO = """Korean business meeting. Clear speech only."""

def fix_base64_padding(base64_str):
    padding = len(base64_str) % 4
    if padding != 0:
        base64_str += '=' * (4 - padding)
    return base64_str

def is_speech(audio_bytes: bytes, sample_rate=16000) -> bool:
    frame_duration = 30
    frame_size = int(sample_rate * frame_duration / 1000) * 2
    for i in range(0, len(audio_bytes), frame_size):
        frame = audio_bytes[i:i + frame_size]
        if len(frame) < frame_size:
            break
        if vad.is_speech(frame, sample_rate):
            return True
    return False

def contains_japanese_chars(text):
    """ì¼ë³¸ì–´ ë¬¸ì í¬í•¨ ì—¬ë¶€ ê°•ë ¥ ê²€ì¦"""
    if not text:
        return False

    for char in text:
        char_code = ord(char)
        if ((0x3040 <= char_code <= 0x309F) or  # íˆë¼ê°€ë‚˜
            (0x30A0 <= char_code <= 0x30FF) or  # ê°€íƒ€ì¹´ë‚˜
            (0xFF66 <= char_code <= 0xFF9F)):   # ë°˜ê° ê°€íƒ€ì¹´ë‚˜
            return True

    return False

def calculate_text_quality(text):
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
    if not text:
        return 0

    quality_score = 0

    # 1. ê¸¸ì´ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
    length = len(text.strip())
    if 5 <= length <= 100:
        quality_score += 30
    elif length > 100:
        quality_score += 20
    elif length >= 3:
        quality_score += 15

    # 2. ì˜ë¯¸ìˆëŠ” ë¬¸ì ë¹„ìœ¨
    korean_chars = len([c for c in text if '\uAC00' <= c <= '\uD7A3'])
    english_chars = len([c for c in text if c.isalpha() and ord(c) < 256])
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    total_meaningful = korean_chars + english_chars + chinese_chars

    if len(text) > 0:
        meaningful_ratio = total_meaningful / len(text)
        quality_score += meaningful_ratio * 40

    # 3. ë°˜ë³µ íŒ¨í„´ ê°ì 
    if re.search(r'(.)\1{3,}', text):  # ê°™ì€ ë¬¸ì 4ê°œ ì´ìƒ ë°˜ë³µ
        quality_score -= 20

    # 4. ì¼ë³¸ì–´ ë¬¸ì ê°ì 
    if contains_japanese_chars(text):
        quality_score -= 50

    # 5. íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ êµ¬ì„± ê°ì 
    if re.match(r'^\W+$', text.strip()):
        quality_score -= 30

    return max(0, min(100, quality_score))

async def two_pass_transcribe(audio_np):
    """
    2-Pass STT: 1ì°¨ ìë™ê°ì§€ â†’ ì¼ë³¸ì–´ë©´ 2ì°¨ í•œêµ­ì–´ ê°•ì œ

    Args:
        audio_np: ì˜¤ë””ì˜¤ ë°ì´í„°

    Returns:
        tuple: (best_text, best_lang, pass_info)
    """

    # ğŸ¯ 1ì°¨ ì‹œë„: ìë™ ì–¸ì–´ ê°ì§€
    print("ğŸ”„ 1ì°¨ STT ì‹œë„: ìë™ ì–¸ì–´ ê°ì§€")
    try:
        segments_1st, info_1st = model.transcribe(
            audio_np,
            beam_size=1,
            vad_filter=True,
            temperature=0.0,
            initial_prompt=WHISPER_PROMPT_AUTO
        )

        first_text = ""
        for segment in segments_1st:
            if segment.text.strip():
                first_text = segment.text.strip()
                break

        detected_lang_1st = info_1st.language
        quality_1st = calculate_text_quality(first_text)

        print(f"   ğŸ“Š 1ì°¨ ê²°ê³¼: '{first_text}' (ì–¸ì–´: {detected_lang_1st}, í’ˆì§ˆ: {quality_1st})")

        # ğŸš¨ ì¼ë³¸ì–´ ê°ì§€ ë˜ëŠ” ì¼ë³¸ì–´ ë¬¸ì í¬í•¨ ì‹œ 2ì°¨ ì‹œë„
        if (detected_lang_1st in ["ja", "japanese"] or
            contains_japanese_chars(first_text) or
            quality_1st < 30):  # í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ì•„ë„ ì¬ì‹œë„

            print("ğŸ”„ 2ì°¨ STT ì‹œë„: í•œêµ­ì–´ ê°•ì œ ì§€ì •")

            try:
                segments_2nd, info_2nd = model.transcribe(
                    audio_np,
                    beam_size=1,
                    vad_filter=True,
                    temperature=0.0,
                    language="ko",  # ğŸ”’ í•œêµ­ì–´ ê°•ì œ
                    initial_prompt=WHISPER_PROMPT_KO
                )

                second_text = ""
                for segment in segments_2nd:
                    if segment.text.strip():
                        second_text = segment.text.strip()
                        break

                detected_lang_2nd = info_2nd.language
                quality_2nd = calculate_text_quality(second_text)

                print(f"   ğŸ“Š 2ì°¨ ê²°ê³¼: '{second_text}' (ì–¸ì–´: {detected_lang_2nd}, í’ˆì§ˆ: {quality_2nd})")

                # ğŸ¯ ë” ì¢‹ì€ ê²°ê³¼ ì„ íƒ
                if quality_2nd > quality_1st and not contains_japanese_chars(second_text):
                    print(f"   âœ… 2ì°¨ ê²°ê³¼ ì±„íƒ (í’ˆì§ˆ í–¥ìƒ: {quality_1st} â†’ {quality_2nd})")
                    return second_text, detected_lang_2nd, "2nd-pass-korean"
                else:
                    print(f"   âš ï¸ 2ì°¨ ê²°ê³¼ë„ ë¶ˆë§Œì¡±, 1ì°¨ ê²°ê³¼ ìœ ì§€")

            except Exception as e:
                print(f"   âŒ 2ì°¨ STT ì‹¤íŒ¨: {e}")

        # 1ì°¨ ê²°ê³¼ ì‚¬ìš©
        return first_text, detected_lang_1st, "1st-pass-auto"

    except Exception as e:
        print(f"âŒ 1ì°¨ STT ì‹¤íŒ¨: {e}")
        return "", "unknown", "failed"

def detect_language_from_text(text):
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ê°ì§€"""
    if not text:
        return "unknown"

    if contains_japanese_chars(text):
        return "ja"

    korean_chars = len([c for c in text if '\uAC00' <= c <= '\uD7A3' or '\u3131' <= c <= '\u318E'])
    english_chars = len([c for c in text if c.isalpha() and ord(c) < 256])
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])

    total_chars = korean_chars + english_chars + chinese_chars

    if total_chars == 0:
        return "unknown"

    korean_ratio = korean_chars / total_chars
    english_ratio = english_chars / total_chars
    chinese_ratio = chinese_chars / total_chars

    threshold = 0.3

    if korean_ratio >= threshold:
        return "ko"
    elif english_ratio >= threshold:
        return "en"
    elif chinese_ratio >= threshold:
        return "zh"
    else:
        return "unknown"

def check_language_validity(detected_lang, text):
    """ì–¸ì–´ ìœ íš¨ì„± ê²€ì‚¬ - ì¼ë³¸ì–´ëŠ” ë¬´ì¡°ê±´ ì°¨ë‹¨"""
    text_based_lang = detect_language_from_text(text)

    print(f"ğŸ” ì–¸ì–´ ë¶„ì„: STTê°ì§€={detected_lang}, í…ìŠ¤íŠ¸ë¶„ì„={text_based_lang}")

    # ğŸš¨ ì¼ë³¸ì–´ëŠ” ë¬´ì¡°ê±´ ì°¨ë‹¨
    if text_based_lang == "ja":
        print(f"ğŸš« ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ê°ì§€ â†’ ì™„ì „ ì°¨ë‹¨")
        return False, None

    if detected_lang == "ja" or detected_lang == "japanese":
        print(f"ğŸš« STTì—ì„œ ì¼ë³¸ì–´ ê°ì§€ â†’ ì™„ì „ ì°¨ë‹¨")
        return False, None

    # í—ˆìš©ëœ ì–¸ì–´ ì²˜ë¦¬
    if text_based_lang in ["ko", "en", "zh"]:
        return True, text_based_lang

    allowed_stt_languages = ["ko", "korean", "en", "english", "zh", "chinese", "zh-Hans"]
    if detected_lang in allowed_stt_languages:
        lang_mapping = {"korean": "ko", "english": "en", "chinese": "zh", "zh-Hans": "zh"}
        final_lang = lang_mapping.get(detected_lang, detected_lang)
        return True, final_lang

    return False, None

def is_valid_content(text):
    """ìœ íš¨í•œ ìë§‰ ë‚´ìš©ì¸ì§€ ê²€ì¦"""
    if not text or len(text.strip()) < 2:
        return False

    if contains_japanese_chars(text):
        return False

    # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ë“¤...
    quality = calculate_text_quality(text)
    return quality >= 30

@app.websocket("/ws/whisper")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    last_text = None

    try:
        while True:
            data = await websocket.receive_json()
            audio_b64 = data["audioData"]
            meeting_id = data["meetingId"]
            speaker = data["speaker"]
            chunk_start_time = float(data.get("chunkStartTime", datetime.now().timestamp()))

            # Base64 ë””ì½”ë”©
            audio_b64 = fix_base64_padding(audio_b64)
            audio_bytes = base64.b64decode(audio_b64)

            if len(audio_bytes) % 2 != 0:
                audio_bytes = audio_bytes[:-1]

            # ë¬µìŒ ì²´í¬
            if not is_speech(audio_bytes):
                continue

            audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0

            # ğŸ¯ 2-Pass STT ì‹¤í–‰
            text, detected_lang, pass_info = await two_pass_transcribe(audio_np)

            if not text:
                print("ğŸš« STT ê²°ê³¼ ì—†ìŒ")
                continue

            print(f"ğŸ¤ STT ìµœì¢… ê²°ê³¼: '{text}' (ì–¸ì–´: {detected_lang}, ë°©ì‹: {pass_info})")

            # ì¼ë³¸ì–´ ë¬¸ì í¬í•¨ ì‹œ ì¦‰ì‹œ ì°¨ë‹¨
            if contains_japanese_chars(text):
                print(f"ğŸš« ì¼ë³¸ì–´ ë¬¸ì ê°ì§€ â†’ ìë§‰ ì°¨ë‹¨: '{text}'")
                continue

            # ì–¸ì–´ ìœ íš¨ì„± ê²€ì‚¬
            is_valid_lang, adjusted_lang = check_language_validity(detected_lang, text)

            if not is_valid_lang:
                print("ğŸš« í—ˆìš©ë˜ì§€ ì•Šì€ ì–¸ì–´ â†’ ì „ì†¡ ì°¨ë‹¨!")
                continue

            # í™˜ê° ì œê±°
            cleaned_text = remover.remove_hallucinations(text)
            if not cleaned_text:
                print("ğŸš« í™˜ê° ì œê±° í›„ ë¹ˆ í…ìŠ¤íŠ¸")
                continue

            print(f"ğŸ§¹ í™˜ê° ì œê±° í›„: '{cleaned_text}'")

            # í™˜ê° ì œê±° í›„ì—ë„ ì¼ë³¸ì–´ ë¬¸ì ì²´í¬
            if contains_japanese_chars(cleaned_text):
                print(f"ğŸš« í™˜ê° ì œê±° í›„ì—ë„ ì¼ë³¸ì–´ ë¬¸ì ê°ì§€ â†’ ì°¨ë‹¨: '{cleaned_text}'")
                continue

            if any(re.search(pattern, cleaned_text, re.IGNORECASE) for pattern in BLOCK_PATTERNS):
                print(f"ğŸš« ê¸ˆì§€ëœ ë‹¨ì–´ í¬í•¨ â†’ ìë§‰ ì°¨ë‹¨: '{cleaned_text}'")
                continue

            # ì¤‘ë³µ ì²´í¬
            if cleaned_text == last_text:
                print(f"ğŸš« ì¤‘ë³µ í…ìŠ¤íŠ¸ ìŠ¤í‚µ: '{cleaned_text}'")
                continue

            # ìœ íš¨ì„± ê²€ì¦
            if not is_valid_content(cleaned_text):
                print("ğŸš« ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨ â†’ ì „ì†¡ ì°¨ë‹¨!")
                continue

            last_text = cleaned_text

            utterance_id = uuid.uuid4().hex
            segment_start = chunk_start_time
            timestamp = datetime.fromtimestamp(segment_start).strftime('%Y-%m-%dT%H:%M:%S')

            # ì›¹ì†Œì¼“ ì „ì†¡
            print(f"ğŸ“± ì›¹ì†Œì¼“ ì „ì†¡: '{cleaned_text}' (ë°©ì‹: {pass_info})")
            # ìë§‰ ë¨¼ì € ì „ì†¡
            await websocket.send_json({"text": cleaned_text})

            # ë²ˆì—­ ë° Spring ì „ì†¡ì€ ë¹„ë™ê¸°ë¡œ
            asyncio.create_task(translate_and_resend(
                cleaned_text, adjusted_lang, speaker, meeting_id, utterance_id, timestamp
            ))

            # íˆ¬ë‘ ê°ì§€ë„ ë¹„ë™ê¸°ë¡œ ë¶„ë¦¬!
            if any(kw in cleaned_text.lower() for kw in TODO_KEYWORDS):
                print("ğŸ“Œ íˆ¬ë‘ ê°ì§€ë¨! ë°±ì—”ë“œ í˜¸ì¶œ ì˜ˆì •")
                asyncio.create_task(send_todo_request(meeting_id))

    except Exception as e:
        print("âŒ ì—°ê²° ì¢…ë£Œ:", e)

async def send_todo_request(meeting_id):
    try:
        await asyncio.to_thread(requests.post, f"https://3.34.92.187.nip.io/api/llm/todo/extract/{meeting_id}", timeout=5)
        print("âœ… íˆ¬ë‘ ìš”ì²­ ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ TODO ìš”ì²­ ì‹¤íŒ¨: {e}")


async def translate_and_resend(text, lang, speaker, meeting_id, utterance_id, timestamp):
    """ë²ˆì—­ ë° Spring Boot ì „ì†¡"""
    try:
        # ë²ˆì—­ ì „ ìµœì¢… ì¼ë³¸ì–´ ì²´í¬
        if contains_japanese_chars(text):
            print(f"ğŸš« ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì¼ë³¸ì–´ ë¬¸ì ê°ì§€ â†’ ì „ì†¡ ì°¨ë‹¨: '{text}'")
            return

        url = AZURE_TRANSLATOR_ENDPOINT + "/translate"
        headers = {
            "Ocp-Apim-Subscription-Key": AZURE_TRANSLATOR_KEY,
            "Ocp-Apim-Subscription-Region": AZURE_TRANSLATOR_REGION,
            "Content-type": "application/json"
        }

        target_langs = ["ko", "en", "zh-Hans"]
        if lang in target_langs:
            target_langs.remove(lang)

        params = {"api-version": "3.0", "from": lang, "to": target_langs}
        body = [{"text": text}]
        res = requests.post(url, params=params, headers=headers, json=body, timeout=10)
        res.raise_for_status()

        translations = res.json()[0]["translations"]
        result = {lang: text}
        for t in translations:
            if t["to"] == "zh-Hans":
                result["zh"] = t["text"]
            else:
                result[t["to"]] = t["text"]

        payload = {
            "meetingId": meeting_id, "speaker": speaker, "text": text,
            "textKo": result.get("ko", ""), "textEn": result.get("en", ""),
            "textZh": result.get("zh", ""), "timestamp": timestamp
        }

        print("ğŸ“¤ Spring ì „ì†¡:", {k: v[:30] + "..." if len(str(v)) > 30 else v for k, v in payload.items()})
        requests.post(SPRING_URL, json=payload, timeout=10)
        print("âœ… ë²ˆì—­ ë° ìŠ¤í”„ë§ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")